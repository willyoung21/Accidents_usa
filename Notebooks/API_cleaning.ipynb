{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "In this section, we import the necessary libraries to facilitate the development of the project:\n",
    "\n",
    "- requests: A library used to send HTTP requests to interact with APIs and fetch data from web sources.\n",
    "\n",
    "- pandas: A powerful library for data manipulation and analysis, commonly used to handle structured datasets.\n",
    "\n",
    "- sys: Provides access to system-specific parameters and functions, allowing interaction with the Python runtime environment.\n",
    "\n",
    "- os: A library that facilitates interaction with the operating system, such as reading and writing files or managing directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data from an API and Saving to CSV\n",
    "In this section, we demonstrate how to retrieve data from a public API, process it using Python, and save it to a CSV file for further analysis.\n",
    "\n",
    "#### Dataset URL:\n",
    "\n",
    "We use the NYC Open Data API endpoint to fetch data about motor vehicle collisions in New York City.\n",
    "\n",
    "#### Request Parameters:\n",
    "\n",
    "We limit the response to 200,000 records using the \"$limit\" parameter in the request.\n",
    "\n",
    "#### Steps in the Code:\n",
    "\n",
    "- Send a GET Request: Use the requests.get() method to fetch the data.\n",
    "- Check for Success: If the request is successful (status code 200), convert the response into JSON format.\n",
    "- Create a DataFrame: Load the JSON data into a pandas DataFrame for manipulation and analysis.\n",
    "- Preview Data: Display the first few rows using df.head().\n",
    "- Save to CSV: Save the DataFrame to a CSV file named API_data.csv in the ../data/ directory.\n",
    "\n",
    "#### Error Handling:\n",
    "\n",
    "If the API request fails, the error status code is printed for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                crash_date crash_time           on_street_name  \\\n",
      "0  2021-09-11T00:00:00.000       2:39    WHITESTONE EXPRESSWAY   \n",
      "1  2022-03-26T00:00:00.000      11:45  QUEENSBORO BRIDGE UPPER   \n",
      "2  2022-06-29T00:00:00.000       6:55       THROGS NECK BRIDGE   \n",
      "3  2021-09-11T00:00:00.000       9:35                      NaN   \n",
      "4  2021-12-14T00:00:00.000       8:13          SARATOGA AVENUE   \n",
      "\n",
      "  off_street_name number_of_persons_injured number_of_persons_killed  \\\n",
      "0       20 AVENUE                         2                        0   \n",
      "1             NaN                         1                        0   \n",
      "2             NaN                         0                        0   \n",
      "3             NaN                         0                        0   \n",
      "4  DECATUR STREET                         0                        0   \n",
      "\n",
      "  number_of_pedestrians_injured number_of_pedestrians_killed  \\\n",
      "0                             0                            0   \n",
      "1                             0                            0   \n",
      "2                             0                            0   \n",
      "3                             0                            0   \n",
      "4                             0                            0   \n",
      "\n",
      "  number_of_cyclist_injured number_of_cyclist_killed  ...   latitude  \\\n",
      "0                         0                        0  ...        NaN   \n",
      "1                         0                        0  ...        NaN   \n",
      "2                         0                        0  ...        NaN   \n",
      "3                         0                        0  ...  40.667202   \n",
      "4                         0                        0  ...  40.683304   \n",
      "\n",
      "    longitude                                           location  \\\n",
      "0         NaN                                                NaN   \n",
      "1         NaN                                                NaN   \n",
      "2         NaN                                                NaN   \n",
      "3    -73.8665  {'latitude': '40.667202', 'longitude': '-73.86...   \n",
      "4  -73.917274  {'latitude': '40.683304', 'longitude': '-73.91...   \n",
      "\n",
      "         cross_street_name contributing_factor_vehicle_3 vehicle_type_code_3  \\\n",
      "0                      NaN                           NaN                 NaN   \n",
      "1                      NaN                           NaN                 NaN   \n",
      "2                      NaN                           NaN                 NaN   \n",
      "3  1211      LORING AVENUE                           NaN                 NaN   \n",
      "4                      NaN                           NaN                 NaN   \n",
      "\n",
      "  contributing_factor_vehicle_4 vehicle_type_code_4  \\\n",
      "0                           NaN                 NaN   \n",
      "1                           NaN                 NaN   \n",
      "2                           NaN                 NaN   \n",
      "3                           NaN                 NaN   \n",
      "4                           NaN                 NaN   \n",
      "\n",
      "  contributing_factor_vehicle_5 vehicle_type_code_5  \n",
      "0                           NaN                 NaN  \n",
      "1                           NaN                 NaN  \n",
      "2                           NaN                 NaN  \n",
      "3                           NaN                 NaN  \n",
      "4                           NaN                 NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# URL of the dataset (API endpoint)\n",
    "url = \"https://data.cityofnewyork.us/resource/h9gi-nx95.json\"\n",
    "\n",
    "# Parameters to limit the response to 200,000 records\n",
    "params = {\n",
    "    \"$limit\": 200000\n",
    "}\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Convert the response to JSON format\n",
    "    df = pd.DataFrame(data)  # Create a pandas DataFrame from the data\n",
    "    print(df.head())  # Display the first few records\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('../data/API_data.csv', index=False, encoding='utf-8')\n",
    "else:\n",
    "    # If the request fails, print the error code\n",
    "    print(f\"Error in the request: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "This section focuses on preparing the dataset for analysis by cleaning and filtering the data. The steps include loading the data, handling missing values, correcting inconsistencies, and removing unnecessary columns.\n",
    "\n",
    "#### Steps in the Code:\n",
    "\n",
    "1. Load the Dataset:\n",
    "\n",
    "- Retrieve the absolute file path for API_data.csv.\n",
    "- Use pandas to load the CSV file into a DataFrame.\n",
    "\n",
    "2. Display Settings:\n",
    "\n",
    "- Configure pandas to display all columns to ensure comprehensive visualization during analysis.\n",
    "\n",
    "3. Convert Columns to DateTime Format:\n",
    "\n",
    "- Convert the crash_date and crash_time columns to datetime format using the pd.to_datetime() method.\n",
    "- Handle invalid parsing by setting such values to NaT (Not a Time).\n",
    "\n",
    "4. Fix Inconsistent Values:\n",
    "\n",
    "- Standardize the borough column by removing extra spaces and capitalizing the first letter of each word.\n",
    "\n",
    "5. Filter and Clean Data:\n",
    "\n",
    "- Keep only rows with valid crash_date values from the year 2021 or later.\n",
    "- Remove duplicates based on the collision_id column, which is assumed to uniquely identify each collision.\n",
    "\n",
    "6. Column Adjustments:\n",
    "\n",
    "- Simplify the crash_date column to include only the date (drop the time part).\n",
    "- Remove unnecessary columns such as details for additional vehicles and contributing factors.\n",
    "\n",
    "7. Handle Missing Values:\n",
    "\n",
    "- Drop rows containing any missing values to ensure data integrity.\n",
    "\n",
    "8. Add a New Column:\n",
    "\n",
    "- Insert a city column with the value \"New York\" for all rows.\n",
    "\n",
    "9. Summary Information:\n",
    "\n",
    "- Display the total number of null and duplicate values for quality checks.\n",
    "\n",
    "10. Save Cleaned Data:\n",
    "\n",
    "- Export the cleaned dataset to a new CSV file, API_data_Cleaned.csv, for further analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTERED AND CLEANED DATA: \n",
      "\n",
      "The total of Null data is: \n",
      "crash_date                       0\n",
      "crash_time                       0\n",
      "on_street_name                   0\n",
      "off_street_name                  0\n",
      "number_of_persons_injured        0\n",
      "number_of_persons_killed         0\n",
      "number_of_pedestrians_injured    0\n",
      "number_of_pedestrians_killed     0\n",
      "number_of_cyclist_injured        0\n",
      "number_of_cyclist_killed         0\n",
      "number_of_motorist_injured       0\n",
      "number_of_motorist_killed        0\n",
      "contributing_factor_vehicle_1    0\n",
      "contributing_factor_vehicle_2    0\n",
      "collision_id                     0\n",
      "vehicle_type_code1               0\n",
      "vehicle_type_code2               0\n",
      "borough                          0\n",
      "zip_code                         0\n",
      "latitude                         0\n",
      "longitude                        0\n",
      "location                         0\n",
      "city                             0\n",
      "dtype: int64\n",
      "\n",
      "The total of duplicated data is: 0\n",
      "\n",
      "Data: 46438 rows\n",
      "\n",
      "File Cleaned Successfully\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the file\n",
    "file_path = os.path.abspath(os.path.join('../data/API_data.csv'))\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Set pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 2. Convert `crash_date` and `crash_time` to datetime format\n",
    "# Handle errors by setting invalid parsing as NaT (Not a Time)\n",
    "data['crash_date'] = pd.to_datetime(data['crash_date'], errors='coerce')\n",
    "data['crash_time'] = pd.to_datetime(data['crash_time'], format='%H:%M', errors='coerce')\n",
    "\n",
    "# 3. Fix inconsistent values (e.g., remove whitespace or correct capitalization in the `borough` column)\n",
    "data['borough'] = data['borough'].str.strip().str.title()\n",
    "\n",
    "# Filter data to keep only rows with valid crash dates and from the year 2021 or later\n",
    "data = data[data['crash_date'].notna() & (data['crash_date'].dt.year >= 2021)]\n",
    "\n",
    "# 4. Remove duplicates based on the `collision_id` column (assuming it's unique for each accident)\n",
    "data = data.drop_duplicates(subset='collision_id')\n",
    "\n",
    "# Convert `crash_date` to just the date (drop the time part)\n",
    "data['crash_date'] = data['crash_date'].dt.date\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(['vehicle_type_code_5', 'contributing_factor_vehicle_5',\n",
    "                  'vehicle_type_code_4', 'contributing_factor_vehicle_4',\n",
    "                  'vehicle_type_code_3', 'contributing_factor_vehicle_3',\n",
    "                  'cross_street_name'], axis=1)\n",
    "\n",
    "print(\"FILTERED AND CLEANED DATA: \\n\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Add a `city` column with the value \"New York\"\n",
    "data['city'] = \"New York\"\n",
    "\n",
    "# Print summary information about null values and duplicates\n",
    "print(f\"The total of Null data is: \\n{data.isnull().sum()}\\n\")\n",
    "print(f\"The total of duplicated data is: {data.duplicated().sum()}\\n\")\n",
    "print(f\"Data: {data.shape[0]} rows\\n\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data.to_csv('../data/API_data_Cleaned.csv', index=False, encoding='utf-8')\n",
    "print(\"File Cleaned Successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
